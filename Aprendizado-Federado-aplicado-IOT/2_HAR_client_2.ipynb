{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj3F1nTZj1ho"
   },
   "source": [
    "# Hands On - Aprendizado Federado aplicado à Internet das Coisas\n",
    "\n",
    "**Notebook 2**: Criação de clientes no ambiente federado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJPTVnswkMdN"
   },
   "source": [
    "O reconhecimento da atividade humana é uma área de pesquisa ativa e que possui um enorme potencial de benefício com o uso de aprendizado federado (FL), já que tais dados são normalmente privados e possuem informações sensíveis sobre os usuários.\n",
    "Além disso, com FL também podemos desenvolver um modelo conjunto que consiga capturar a diversidade dos dados, algo que é extremamente difícil de ser coletado de forma individual.\n",
    "\n",
    "Sob esse contexto, nesse tutorial vamos aprender como definir clientes para o treinamento federado de uma rede neural para auxilar no reconhecimento de atividades humanas (*Human Activity Recognition* - HAR) usando o framework de aprendizado federado\n",
    "Flower em conjunto com a biblioteca de deep learning Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hX7rxsAk8CT"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Os dados serão particionados horizontalmente, assim os subconjuntos de treinamento e teste irão ser divididos em mini-batches (pequenos lotes) com base no número total de clientes.\n",
    "\n",
    "Para isso, aplicaremos uma função auxiliar para carregar os dados e definir os conjuntos de treinamento e teste.\n",
    "Nessa função, precisaremos dos seguintes parâmetros: \n",
    "\n",
    "* **data root (str)**: Diretório onde os datasets finais serão armazenados. \n",
    "\n",
    "* **train batch size (int)**: Tamanho do mini-batch usado nos dados de treinamento.\n",
    "\n",
    "* **test batch size (int)**: Tamanho do mini-batch usado nos dados de teste. \n",
    "\n",
    "* **id (int)**: Client ID usado para selecionar uma partição específica. \n",
    "\n",
    "* **nb clients (int)**: Número total de clientes usados no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVQzVKL2r67J",
    "outputId": "67066d3f-f7bb-412c-e76b-091aea68ff3c"
   },
   "outputs": [],
   "source": [
    "#Carregando os dados\n",
    "import flwr as fl\n",
    "import torch\n",
    "import aux\n",
    "\n",
    "DATA_ROOT = \"./data/pml-training.csv\"\n",
    "\n",
    "cid = 1\n",
    "nb_clients = 3\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Load data\n",
    "train_loader, test_loader = aux.load_data(\n",
    "        data_root = DATA_ROOT,\n",
    "        train_batch_size = train_batch_size,\n",
    "        test_batch_size = test_batch_size,\n",
    "        cid = cid,\n",
    "        nb_clients = nb_clients + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arvhg44xrwWm"
   },
   "source": [
    "### Rede Neural\n",
    "\n",
    "Atualmente o modelo de classificação mais adequado e vantajoso para a modelagem de um ambiente federado são as redes neurais.\n",
    "Definimos essa configuração de arquitetura por meio da criação de uma classe em Pytorch denominada **HARmodel** presente no arquivo auxiliar *aux.py* adicionado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-glqWQnYsGt3"
   },
   "source": [
    "### Cliente Flower\n",
    "\n",
    "O próximo passo é definir a alocação dos dispositivos no ambiente federado. \n",
    "\n",
    "Quando o servidor seleciona um dispositivo específico do ambiente federado para realizar um treinamento, ele envia as instruções pela rede, por meio de uma interface chamada **Client**.\n",
    "Assim, o cliente recebe as instruções do servidor e chama um dos métodos desta classe para executar seu código (ou seja, para treinar a sua rede neural local). \n",
    "\n",
    "O framework Flower fornece uma classe chamada *NumPyClient*, que torna mais fácil implementar a interface do cliente quando utilizamos PyTorch. \n",
    "Quando implementamos um NumPyClient devemos definir os seguintes métodos: \n",
    "\n",
    "* **get_parameters**: retorna o peso do modelo\n",
    "como uma lista de ndarrays \n",
    "\n",
    "* **set_parameters** (opcional): atualiza os pesos do modelo\n",
    "local com os parâmetros recebidos do servidor \n",
    "\n",
    "* **fit**: define os pesos do modelo local, treina o modelo localmente e recebe o update dos pesos locais \n",
    "\n",
    "* **evaluate**: define como o modelo local será testado. \n",
    "\n",
    "Abaixo mostramos como a classe Client foi implementada\n",
    "para o caso de estudo apresentado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.Client):\n",
    "    \"\"\"Flower client implementing classification using PyTorch.\"\"\"\n",
    "\n",
    "    def __init__(self, cid, train_loader, test_loader, epochs, device: torch.device = torch.device(\"cpu\")):\n",
    "        \n",
    "        self.model = HARmodel(40, 5).to(device)\n",
    "        self.cid = cid\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"Get model weights as a list of NumPy ndarrays.\"\"\"\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"Set model weights from a list of NumPy ndarrays.\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: fl.common.Weights\n",
    "            Weights received by the server and set to local model\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        state_dict = OrderedDict(\n",
    "            {\n",
    "                k: torch.Tensor(v)\n",
    "                for k, v in zip(self.model.state_dict().keys(), weights)\n",
    "            }\n",
    "        )\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"Encapsulates the weights into Flower Parameters \"\"\"\n",
    "        weights: fl.common.Weights = self.get_weights()\n",
    "        parameters = fl.common.weights_to_parameters(weights)\n",
    "        return fl.common.ParametersRes(parameters=parameters)\n",
    "\n",
    "    def fit(self, ins):\n",
    "        \"\"\"Trains the model on local dataset\n",
    "        Parameters\n",
    "        ----------\n",
    "        ins: fl.common.FitIns\n",
    "           Parameters sent by the server to be used during training.\n",
    "        Returns\n",
    "        -------\n",
    "            Set of variables containing the new set of weights and information the client.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the seed so we are sure to generate the same global batches\n",
    "        # indices across all clients\n",
    "        np.random.seed(123)\n",
    "\n",
    "        weights: fl.common.Weights = fl.common.parameters_to_weights(ins.parameters)\n",
    "        fit_begin = timeit.default_timer()\n",
    "\n",
    "        # Set model parameters/weights\n",
    "        self.set_weights(weights)\n",
    "\n",
    "        # Train model\n",
    "        num_examples_train: int = train(\n",
    "            self.model, self.train_loader, epochs = self.epochs, device = self.device, cid = self.cid\n",
    "        )\n",
    "\n",
    "        # Return the refined weights and the number of examples used for training\n",
    "        weights_prime: fl.common.Weights = self.get_weights()\n",
    "        params_prime = fl.common.weights_to_parameters(weights_prime)\n",
    "        fit_duration = timeit.default_timer() - fit_begin\n",
    "        return fl.common.FitRes(\n",
    "            parameters = params_prime,\n",
    "            num_examples = num_examples_train,\n",
    "            num_examples_ceil = num_examples_train,\n",
    "            fit_duration = fit_duration,\n",
    "        )\n",
    "\n",
    "    def evaluate(self, ins):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ins: fl.common.EvaluateIns\n",
    "           Parameters sent by the server to be used during testing.\n",
    "        Returns\n",
    "        -------\n",
    "            Information the clients testing results.\n",
    "        \"\"\"\n",
    "        weights = fl.common.parameters_to_weights(ins.parameters)\n",
    "\n",
    "        # Use provided weights to update the local model\n",
    "        self.set_weights(weights)\n",
    "\n",
    "        (num_examples_test, test_loss, accuracy) = test(self.model, self.test_loader, device = self.device)\n",
    "        print(f\"Client {self.cid} - Evaluate on {num_examples_test} samples: Average loss: {test_loss:.4f}, Accuracy: {100*accuracy:.2f}%\\n\")\n",
    "\n",
    "        # Return the number of evaluation examples and the evaluation result (loss)\n",
    "        return fl.common.EvaluateRes(\n",
    "            num_examples = num_examples_test,\n",
    "            loss = float(test_loss),\n",
    "            accuracy = float(accuracy),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciando o cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MEUIciNJ69re"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "client = aux.FlowerClient(\n",
    "    cid = cid,\n",
    "    train_loader = train_loader,\n",
    "    test_loader = test_loader,\n",
    "    epochs = epochs,\n",
    "    device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNxtUr3s67hn"
   },
   "source": [
    "### Inicializando o cliente\n",
    "\n",
    "O flower nos fornece a possibilidade de rodar o servidor e o cliente na mesma máquina, configurando o endereço do servidor como \"[::]: 8080\". \n",
    "Porém, se quisermos implementar uma aplicação realmente federada com o servidor e clientes em execução em diferentes máquinas, precisaremos apenas alterar o server address para o respectivo endereço da máquina do cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CD9ie8II7QHB",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2021-11-15 16:29:35,542 | connection.py:36 | ChannelConnectivity.IDLE\n",
      "DEBUG flower 2021-11-15 16:29:35,544 | connection.py:36 | ChannelConnectivity.CONNECTING\n",
      "INFO flower 2021-11-15 16:29:35,544 | app.py:61 | Opened (insecure) gRPC connection\n",
      "DEBUG flower 2021-11-15 16:29:35,545 | connection.py:36 | ChannelConnectivity.READY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 10 epoch(s) w/ 77 mini-batches each\n",
      "\n",
      "Train Epoch: 0 [4416/4928 (90%)] Loss: 1.680233, Acc: 0.230752 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 1 [4416/4928 (90%)] Loss: 1.434450, Acc: 0.277853 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 2 [4416/4928 (90%)] Loss: 1.551119, Acc: 0.321332 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 3 [4416/4928 (90%)] Loss: 1.398273, Acc: 0.348279 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 4 [4416/4928 (90%)] Loss: 1.297337, Acc: 0.379982 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 5 [4416/4928 (90%)] Loss: 1.252618, Acc: 0.432518 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 6 [4416/4928 (90%)] Loss: 1.011664, Acc: 0.447690 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 7 [4416/4928 (90%)] Loss: 1.076121, Acc: 0.474638 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 8 [4416/4928 (90%)] Loss: 1.287127, Acc: 0.487092 (Cliente 1)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 77 mini-batches each641, Acc: 0.496150 (Cliente 1)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [4416/4928 (90%)] Loss: 1.091125, Acc: 0.489583 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 1 [4416/4928 (90%)] Loss: 1.056695, Acc: 0.524909 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 2 [4416/4928 (90%)] Loss: 1.132869, Acc: 0.523551 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 3 [4416/4928 (90%)] Loss: 1.042107, Acc: 0.544611 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 4 [4416/4928 (90%)] Loss: 1.094880, Acc: 0.545290 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 5 [4416/4928 (90%)] Loss: 1.231815, Acc: 0.563859 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 6 [4416/4928 (90%)] Loss: 0.977026, Acc: 0.568388 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 7 [4416/4928 (90%)] Loss: 1.144105, Acc: 0.574275 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 8 [4416/4928 (90%)] Loss: 1.098296, Acc: 0.579710 (Cliente 1)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 77 mini-batches each602, Acc: 0.580389 (Cliente 1)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [4416/4928 (90%)] Loss: 0.820547, Acc: 0.579031 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 1 [4416/4928 (90%)] Loss: 1.141793, Acc: 0.594656 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 2 [4416/4928 (90%)] Loss: 0.939376, Acc: 0.590806 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 3 [4416/4928 (90%)] Loss: 1.038631, Acc: 0.613904 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 4 [4416/4928 (90%)] Loss: 0.986667, Acc: 0.610507 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 5 [4416/4928 (90%)] Loss: 0.851923, Acc: 0.613225 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 6 [4416/4928 (90%)] Loss: 0.813009, Acc: 0.615263 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 7 [4416/4928 (90%)] Loss: 1.005333, Acc: 0.616848 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 8 [4416/4928 (90%)] Loss: 1.335736, Acc: 0.626585 (Cliente 1)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 77 mini-batches each842, Acc: 0.620018 (Cliente 1)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [4416/4928 (90%)] Loss: 1.124890, Acc: 0.613225 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 1 [4416/4928 (90%)] Loss: 1.042482, Acc: 0.627944 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 2 [4416/4928 (90%)] Loss: 1.072161, Acc: 0.645833 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 3 [4416/4928 (90%)] Loss: 0.831659, Acc: 0.639946 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 4 [4416/4928 (90%)] Loss: 0.778984, Acc: 0.646286 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 5 [4416/4928 (90%)] Loss: 0.904091, Acc: 0.644701 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 6 [4416/4928 (90%)] Loss: 0.918607, Acc: 0.646060 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 7 [4416/4928 (90%)] Loss: 0.933053, Acc: 0.651495 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 8 [4416/4928 (90%)] Loss: 0.831792, Acc: 0.655118 (Cliente 1)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 77 mini-batches each032, Acc: 0.667799 (Cliente 1)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [4416/4928 (90%)] Loss: 1.394181, Acc: 0.650815 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 1 [4416/4928 (90%)] Loss: 0.893114, Acc: 0.657835 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 2 [4416/4928 (90%)] Loss: 0.755416, Acc: 0.655797 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 3 [4416/4928 (90%)] Loss: 1.011524, Acc: 0.674819 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 4 [4416/4928 (90%)] Loss: 1.008885, Acc: 0.676630 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 5 [4416/4928 (90%)] Loss: 0.874134, Acc: 0.665534 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 6 [4416/4928 (90%)] Loss: 0.765387, Acc: 0.678668 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 7 [4416/4928 (90%)] Loss: 0.714236, Acc: 0.682292 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 8 [4416/4928 (90%)] Loss: 1.129291, Acc: 0.675045 (Cliente 1)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 77 mini-batches each182, Acc: 0.685915 (Cliente 1)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [4416/4928 (90%)] Loss: 0.939125, Acc: 0.664629 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 1 [4416/4928 (90%)] Loss: 0.672966, Acc: 0.683877 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 2 [4416/4928 (90%)] Loss: 0.809515, Acc: 0.684330 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 3 [4416/4928 (90%)] Loss: 0.960691, Acc: 0.683650 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 4 [4416/4928 (90%)] Loss: 0.655912, Acc: 0.685915 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 5 [4416/4928 (90%)] Loss: 0.714010, Acc: 0.678668 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 6 [4416/4928 (90%)] Loss: 0.649764, Acc: 0.691576 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 7 [4416/4928 (90%)] Loss: 0.633280, Acc: 0.700181 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 8 [4416/4928 (90%)] Loss: 0.904952, Acc: 0.699275 (Cliente 1)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 77 mini-batches each261, Acc: 0.682745 (Cliente 1)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [4416/4928 (90%)] Loss: 0.978211, Acc: 0.688179 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 1 [4416/4928 (90%)] Loss: 0.838064, Acc: 0.699275 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 2 [4416/4928 (90%)] Loss: 0.892323, Acc: 0.690444 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 3 [4416/4928 (90%)] Loss: 0.741758, Acc: 0.704937 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 4 [4416/4928 (90%)] Loss: 0.658596, Acc: 0.696105 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 5 [4416/4928 (90%)] Loss: 0.705063, Acc: 0.700634 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 6 [4416/4928 (90%)] Loss: 0.883100, Acc: 0.693388 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 7 [4416/4928 (90%)] Loss: 1.027701, Acc: 0.698596 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 8 [4416/4928 (90%)] Loss: 0.905133, Acc: 0.691803 (Cliente 1)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 77 mini-batches each181, Acc: 0.707654 (Cliente 1)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [4416/4928 (90%)] Loss: 1.025172, Acc: 0.685915 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 1 [4416/4928 (90%)] Loss: 0.684749, Acc: 0.698143 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 2 [4416/4928 (90%)] Loss: 0.877351, Acc: 0.701087 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 3 [4416/4928 (90%)] Loss: 0.764334, Acc: 0.711051 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 4 [4416/4928 (90%)] Loss: 0.622894, Acc: 0.703125 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 5 [4416/4928 (90%)] Loss: 1.005022, Acc: 0.708333 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 6 [4416/4928 (90%)] Loss: 0.772525, Acc: 0.708786 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 7 [4416/4928 (90%)] Loss: 0.839649, Acc: 0.714221 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 8 [4416/4928 (90%)] Loss: 0.723309, Acc: 0.711504 (Cliente 1)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 77 mini-batches each801, Acc: 0.706748 (Cliente 1)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [4416/4928 (90%)] Loss: 0.835033, Acc: 0.704257 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 1 [4416/4928 (90%)] Loss: 0.750663, Acc: 0.694520 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 2 [4416/4928 (90%)] Loss: 0.702698, Acc: 0.713315 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 3 [4416/4928 (90%)] Loss: 0.798479, Acc: 0.716259 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 4 [4416/4928 (90%)] Loss: 0.702284, Acc: 0.701313 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 5 [4416/4928 (90%)] Loss: 0.830717, Acc: 0.714900 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 6 [4416/4928 (90%)] Loss: 0.875476, Acc: 0.719656 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 7 [4416/4928 (90%)] Loss: 0.790003, Acc: 0.715127 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 8 [4416/4928 (90%)] Loss: 0.950536, Acc: 0.715580 (Cliente 1)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 77 mini-batches each035, Acc: 0.721467 (Cliente 1)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [4416/4928 (90%)] Loss: 0.692479, Acc: 0.711504 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 1 [4416/4928 (90%)] Loss: 0.822063, Acc: 0.718976 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 2 [4416/4928 (90%)] Loss: 0.827157, Acc: 0.712409 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 3 [4416/4928 (90%)] Loss: 1.079071, Acc: 0.705389 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 4 [4416/4928 (90%)] Loss: 0.740275, Acc: 0.717165 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 5 [4416/4928 (90%)] Loss: 0.988152, Acc: 0.724411 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 6 [4416/4928 (90%)] Loss: 0.934089, Acc: 0.721694 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 7 [4416/4928 (90%)] Loss: 0.872156, Acc: 0.709466 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 8 [4416/4928 (90%)] Loss: 0.604945, Acc: 0.719429 (Cliente 1)\t\t\t\t\n",
      "Train Epoch: 9 [4416/4928 (90%)] Loss: 0.867617, Acc: 0.728714 (Cliente 1)\t\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2021-11-15 16:33:18,461 | connection.py:68 | Insecure gRPC channel closed\n",
      "INFO flower 2021-11-15 16:33:18,462 | app.py:72 | Disconnect and shut down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 - Evaluate on 4905 samples: Average loss: 0.0086, Accuracy: 79.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_address = \"[::]:8081\"\n",
    "fl.client.start_client(client_address, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2-HAR-client_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
